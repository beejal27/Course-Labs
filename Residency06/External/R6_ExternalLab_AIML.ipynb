{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYk8NG3yOIT9"
   },
   "source": [
    "### A MNIST-like fashion product database\n",
    "\n",
    "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFO6PuxzOIT_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efNjNImfOIUC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9C4aAIGOIUH",
    "outputId": "5ef9aff6-a7bd-4b26-cba6-8750955f6ca3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1566710721.7026007"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions - START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(r):\n",
    "    #print(r)\n",
    "    max = r[0]\n",
    "    idx = 0\n",
    "    for i in range(1, len(r)):\n",
    "        if r[i] > max:\n",
    "            max = r[i]\n",
    "            idx = i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions - END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcoZBStrOIUQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnbx7TyQOIUY"
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Information About Fashion-MNIST database of fashion articles__\n",
    "\n",
    "Dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "    Label\tDescription\n",
    "        0\tT-shirt/top\n",
    "        1\tTrouser\n",
    "        2\tPullover\n",
    "        3\tDress\n",
    "        4\tCoat\n",
    "        5\tSandal\n",
    "        6\tShirt\n",
    "        7\tSneaker\n",
    "        8\tBag\n",
    "        9\tAnkle boot\n",
    "\n",
    "__Usage:__\n",
    "\n",
    "    from keras.datasets import fashion_mnist\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "__Returns:__\n",
    "    2 tuples:\n",
    "        x_train, x_test: uint8 array of grayscale image data with shape (num_samples, 28, 28).\n",
    "        y_train, y_test: uint8 array of labels (integers in range 0-9) with shape (num_samples,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BTW, the 4 data files are downloaded to : C:\\Users\\Beejal\\.keras\\datasets\\fashion-mnist\n",
    "# 1. t10k-images-idx3-ubyte.gz  - testX\n",
    "# 2. t10k-labels-idx1-ubyte.gz  - testY\n",
    "# 3. train-images-idx3-ubyte.gz - trainX\n",
    "# 4. train-labels-idx1-ubyte.gz - trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbiHj5YPOIUc",
    "outputId": "87e1b9cd-07f0-45cb-e706-0d51ad742d72",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, 3, 0], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[0:5]\n",
    "# So the 1st train image is 'Ankle boot', 2nd, 3rd & 5th are 'T-shirt/top', 4th is 'Dress'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAYzkwyOIUj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convert both training and testing labels into one-hot vectors.\n",
    "\n",
    "**Hint:** check **tf.keras.utils.to_categorical()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10  # Since we have 10 classes as output\n",
    "# I create one-hot vectors using get_dummies. So let me continue with that.\n",
    "y_train =  tf.keras.utils.to_categorical(trainY, n_classes)\n",
    "y_test =  tf.keras.utils.to_categorical(testY, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHV3b9mzOIUq",
    "outputId": "27bdfe58-91ee-4677-fe49-e742ad306c70",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now first 5 examples of trainY looks as below: \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Now first 5 examples of trainY looks as below: \\n', y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now first 5 examples of testY looks as below: \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Now first 5 examples of testY looks as below: \\n', y_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhQ8e7VOIUw"
   },
   "source": [
    "### Visualize the data\n",
    "\n",
    "Plot first 10 images in the triaining set and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first understand the datastructure of trainX\n",
    "trainX.shape\n",
    "# So it has got 60,000 records. With each record having 28 rows and 28 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
       "          1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
       "          0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
       "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
       "         10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
       "         72,  15],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
       "        172,  66],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
       "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
       "        229,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
       "        173,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
       "        202,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
       "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
       "        209,  52],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
       "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
       "        167,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0],\n",
       "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
       "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
       "         77,   0],\n",
       "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
       "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
       "        159,   0],\n",
       "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
       "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
       "        215,   0],\n",
       "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
       "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
       "        246,   0],\n",
       "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
       "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
       "        225,   0],\n",
       "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
       "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
       "        229,  29],\n",
       "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
       "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
       "        230,  67],\n",
       "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
       "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
       "        206, 115],\n",
       "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
       "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
       "        210,  92],\n",
       "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
       "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
       "        170,   0],\n",
       "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
       "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see how an image record looks like. Given images are grayscale images so each value in the given record\n",
    "# represents the brightness of the pixel. The most common pixel format is the byte image, \n",
    "# where this number is stored as an 8-bit integer giving a range of possible values from 0 to 255. \n",
    "# Typically zero is taken to be black, and 255 is taken to be white.\n",
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\beejal\\anaconda3\\lib\\site-packages (5.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Python has a library named Pillow (Python Im) https://python-pillow.org/ for working with images. So let's install this library.\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So Pillow is already there. Now let's use it to meet our requirement.\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "#plt.figure(figsize=(8, 8))\n",
    "n = 10\n",
    "plt.subplots(1, n)\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    plt.subplot(1, n, i)\n",
    "    data = trainX[i]\n",
    "    plt.imshow(data, cmap='gray', interpolation='nearest')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transform the Independent Variables.\n",
    "\n",
    "# Each image has 28 rows and 28 columns. Instead of Matrix (2D), if we want flatten it to 1D, then an image can be \n",
    "# represented by 28 * 28 = 784 values.\n",
    "\n",
    "n_inputs = 784\n",
    "\n",
    "x_train = trainX.reshape( trainX.shape[0], n_inputs) \n",
    "x_test = testX.reshape( testX.shape[0], n_inputs) \n",
    "\n",
    "# convert the input values to float32 \n",
    "x_train = x_train.astype( np.float32) \n",
    "x_test = x_test.astype( np.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4TbJGeSOIU4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 10:55:25.040283 13996 deprecation_wrapper.py:119] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0825 10:55:25.042315 13996 deprecation_wrapper.py:119] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0825 10:55:25.045268 13996 deprecation_wrapper.py:119] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0825 10:55:25.063255 13996 deprecation_wrapper.py:119] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0825 10:55:25.071209 13996 deprecation.py:506] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0825 10:55:25.141046 13996 deprecation_wrapper.py:119] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0825 10:55:25.147994 13996 deprecation_wrapper.py:119] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a sequential model \n",
    "\n",
    "model = Sequential() # the first layer has to specify the dimensions of the input vector \n",
    "model.add( Dense( units = 128, activation ='sigmoid', input_shape =( n_inputs,))) # add dropout layer for preventing overfitting \n",
    "model.add( Dropout( 0.1)) \n",
    "model.add( Dense( units = 128, activation ='sigmoid')) \n",
    "model.add( Dropout( 0.1)) # output layer can only have the neurons equal to the number of outputs \n",
    "model.add( Dense( units = n_classes, activation ='sigmoid')) # print the summary of our model \n",
    "model.summary() \n",
    "\n",
    "# compile the model \n",
    "model.compile( loss ='categorical_crossentropy', optimizer = SGD(), metrics =['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQpLv3aOIU_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O59C_-IgOIVB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0825 10:55:25.234892 13996 deprecation.py:323] From C:\\Users\\Beejal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.0899 - acc: 0.3304\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 1.5607 - acc: 0.5750\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 1.1891 - acc: 0.6496\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.9814 - acc: 0.6862\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.8536 - acc: 0.7196\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.7703 - acc: 0.7447\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.7121 - acc: 0.7626\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6665 - acc: 0.7777\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6298 - acc: 0.7885\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6049 - acc: 0.7961\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5870 - acc: 0.7985\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5756 - acc: 0.8014\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5607 - acc: 0.8068\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5526 - acc: 0.8066\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5442 - acc: 0.8117\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5367 - acc: 0.8144\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5299 - acc: 0.8133\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5196 - acc: 0.8182\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5175 - acc: 0.8190\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5156 - acc: 0.8187\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5150 - acc: 0.8203\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5169 - acc: 0.8192\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5117 - acc: 0.8204\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5077 - acc: 0.8209\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5101 - acc: 0.8197\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5130 - acc: 0.8181\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.5051 - acc: 0.8220\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5054 - acc: 0.8211\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4974 - acc: 0.8236\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5033 - acc: 0.8230\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4925 - acc: 0.8259\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4846 - acc: 0.8278\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4879 - acc: 0.8281\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4826 - acc: 0.8299\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4829 - acc: 0.8288\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4873 - acc: 0.8272\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4838 - acc: 0.8281\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4841 - acc: 0.8262\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4825 - acc: 0.8263\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4873 - acc: 0.8260\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4911 - acc: 0.8258\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4841 - acc: 0.8291\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4877 - acc: 0.8271\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4784 - acc: 0.8306\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4791 - acc: 0.8315\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4779 - acc: 0.8290\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4785 - acc: 0.8293\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4782 - acc: 0.8281\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4776 - acc: 0.8300\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4766 - acc: 0.8325\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4800 - acc: 0.8297\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4842 - acc: 0.8272\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4784 - acc: 0.8301\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4847 - acc: 0.8288\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4819 - acc: 0.8286\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4784 - acc: 0.8297\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4783 - acc: 0.8302\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4798 - acc: 0.8274\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4761 - acc: 0.8293\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4682 - acc: 0.8336\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4678 - acc: 0.8327\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4712 - acc: 0.8315\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4650 - acc: 0.8356\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4671 - acc: 0.8343\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4674 - acc: 0.8332\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4630 - acc: 0.8360\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4622 - acc: 0.8359\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4669 - acc: 0.8332\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4593 - acc: 0.8375\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4658 - acc: 0.8330\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4650 - acc: 0.8344\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4599 - acc: 0.8354\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4636 - acc: 0.8340\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4584 - acc: 0.8382\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4584 - acc: 0.8361\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.4589 - acc: 0.8369\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4567 - acc: 0.8371\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4570 - acc: 0.8366\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4522 - acc: 0.8386\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4573 - acc: 0.8364\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4514 - acc: 0.8393\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.4550 - acc: 0.838 - 2s 31us/step - loss: 0.4547 - acc: 0.8382\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4571 - acc: 0.8343\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.4676 - acc: 0.832 - 2s 31us/step - loss: 0.4676 - acc: 0.8328\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4561 - acc: 0.8352\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4593 - acc: 0.8359\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4619 - acc: 0.8343\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4652 - acc: 0.8341\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4530 - acc: 0.8391\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4523 - acc: 0.8397\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4546 - acc: 0.8377\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4497 - acc: 0.8386\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4534 - acc: 0.8400\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4584 - acc: 0.8364\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4596 - acc: 0.8353\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4573 - acc: 0.8381\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4620 - acc: 0.8339\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4573 - acc: 0.8369\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4620 - acc: 0.8364\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4522 - acc: 0.8413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d91d5832e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "model.fit( x_train, y_train, batch_size = batch_size, epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step\n",
      "\\ n loss: 0.45662424952983854\n",
      "\\ n accuracy: 0.8386\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate( x_test, y_test) \n",
    "\n",
    "print('\\ n loss:', scores[ 0])\n",
    "print('\\ n accuracy:', scores[ 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Image:  [9 2 1 1 6]\n",
      "Predicted Image : [9, 2, 1, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual Image: \", testY[0:5])\n",
    "predicted = model.predict(x_test[0:5])\n",
    "print(\"Predicted Image :\", [max_index(p) for p in predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdzDtGwDOIVF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndfpdidOIVI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 121,418\n",
      "Trainable params: 119,850\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a sequential model \n",
    "\n",
    "model = Sequential() # the first layer has to specify the dimensions of the input vector \n",
    "model.add(BatchNormalization(input_shape=(n_inputs,)))\n",
    "model.add( Dense( units = 128, activation ='sigmoid')) # add dropout layer for preventing overfitting \n",
    "model.add( Dropout( 0.1)) \n",
    "model.add( Dense( units = 128, activation ='sigmoid')) \n",
    "model.add( Dropout( 0.1)) # output layer can only have the neurons equal to the number of outputs \n",
    "model.add( Dense( units = n_classes, activation ='sigmoid')) # print the summary of our model \n",
    "model.summary() \n",
    "\n",
    "# compile the model \n",
    "model.compile( loss ='categorical_crossentropy', optimizer = SGD(), metrics =['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwk3T5LJOIVN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNLR8tcBOIVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 2.2280 - acc: 0.2200\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 1.8987 - acc: 0.4642\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 1.4790 - acc: 0.5814\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 1.2049 - acc: 0.6482\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 1.0284 - acc: 0.6851\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.9124 - acc: 0.7068\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.8314 - acc: 0.7211\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7769 - acc: 0.7334\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7380 - acc: 0.7415\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.7038 - acc: 0.7516\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6790 - acc: 0.7560\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6572 - acc: 0.7648\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6379 - acc: 0.7701\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.6176 - acc: 0.7785\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6049 - acc: 0.7824\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.5917 - acc: 0.7886\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.5780 - acc: 0.7934\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5663 - acc: 0.7972\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5600 - acc: 0.8006\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.5487 - acc: 0.8048\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5407 - acc: 0.8068\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5332 - acc: 0.8111\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.5281 - acc: 0.8119\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.5194 - acc: 0.8161\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.5130 - acc: 0.8175\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5079 - acc: 0.8187\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.5032 - acc: 0.8199\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.4977 - acc: 0.8217\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.4931 - acc: 0.8241\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4898 - acc: 0.8266\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4851 - acc: 0.8260\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4813 - acc: 0.8276\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4766 - acc: 0.8315\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4740 - acc: 0.8318\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4737 - acc: 0.8302\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4702 - acc: 0.8332\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4641 - acc: 0.8346\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4613 - acc: 0.8352\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4621 - acc: 0.8363\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4576 - acc: 0.8365\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4565 - acc: 0.8368\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4547 - acc: 0.8388\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.4526 - acc: 0.8387\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4503 - acc: 0.8393\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4470 - acc: 0.8406\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4436 - acc: 0.8424\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4419 - acc: 0.8428\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4392 - acc: 0.8434\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4394 - acc: 0.8434\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4383 - acc: 0.8427\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4367 - acc: 0.8440\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4348 - acc: 0.8442\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4317 - acc: 0.8462\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4296 - acc: 0.8467\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4268 - acc: 0.8469\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4296 - acc: 0.8476\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4273 - acc: 0.8466\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4263 - acc: 0.8471\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4230 - acc: 0.8496\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4215 - acc: 0.8498\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4215 - acc: 0.8490\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4197 - acc: 0.8503\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4169 - acc: 0.8499\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4155 - acc: 0.8522\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4186 - acc: 0.8494\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4131 - acc: 0.8520\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4147 - acc: 0.8497\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4112 - acc: 0.8530\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4104 - acc: 0.8535\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4095 - acc: 0.8547\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4092 - acc: 0.8541\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4071 - acc: 0.8547\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4072 - acc: 0.8537\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4045 - acc: 0.8549\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4038 - acc: 0.8548\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4025 - acc: 0.8560\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4032 - acc: 0.8556\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4028 - acc: 0.8559\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3996 - acc: 0.8574\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.4005 - acc: 0.8573\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3980 - acc: 0.8583\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.3968 - acc: 0.8581\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3943 - acc: 0.8582\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3933 - acc: 0.8588\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3954 - acc: 0.8576\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3939 - acc: 0.8594\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3934 - acc: 0.8584\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3919 - acc: 0.8596\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.3913 - acc: 0.8594\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.3896 - acc: 0.8598\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3905 - acc: 0.8591\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3888 - acc: 0.8606\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3875 - acc: 0.8608\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3859 - acc: 0.8608\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3854 - acc: 0.8617\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3873 - acc: 0.8603\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3846 - acc: 0.8629\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3818 - acc: 0.8627\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3831 - acc: 0.8633\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3813 - acc: 0.8629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9158e0940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "model.fit( x_train, y_train, batch_size = batch_size, epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 42us/step\n",
      "\\ n loss: 0.3916099581003189\n",
      "\\ n accuracy: 0.8581\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate( x_test, y_test) \n",
    "print('\\ n loss:', scores[ 0]) \n",
    "print('\\ n accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Image:  [9 2 1 1 6]\n",
      "Predicted Image : [9, 2, 1, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual Image: \", testY[0:5])\n",
    "predicted = model.predict(x_test[0:5])\n",
    "print(\"Predicted Image :\", [max_index(p) for p in predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-KwkmjOIVU"
   },
   "source": [
    "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLXUE9jWOIVV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 121,418\n",
      "Trainable params: 119,850\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a sequential model \n",
    "lr=0.001\n",
    "model = Sequential() # the first layer has to specify the dimensions of the input vector \n",
    "model.add(BatchNormalization(input_shape=(n_inputs,)))\n",
    "model.add( Dense( units = 128, activation ='sigmoid')) # add dropout layer for preventing overfitting \n",
    "model.add( Dropout( 0.1)) \n",
    "model.add( Dense( units = 128, activation ='sigmoid')) \n",
    "model.add( Dropout( 0.1)) # output layer can only have the neurons equal to the number of outputs \n",
    "model.add( Dense( units = n_classes, activation ='sigmoid')) # print the summary of our model \n",
    "model.summary() \n",
    "\n",
    "# compile the model \n",
    "model.compile( loss ='categorical_crossentropy', optimizer = SGD(lr), metrics =['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJUqA5T4OIVc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.3669 - acc: 0.1172\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 2.3015 - acc: 0.1233\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 2.2677 - acc: 0.1352\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.2429 - acc: 0.1615\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 2.2196 - acc: 0.2060\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 2.1968 - acc: 0.2440\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 2.1705 - acc: 0.2813\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 2.1426 - acc: 0.3104\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 2.1086 - acc: 0.3401\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 2.0707 - acc: 0.3677\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 2.0287 - acc: 0.3924\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.9845 - acc: 0.4152\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.9363 - acc: 0.4341\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.8875 - acc: 0.4483\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.8376 - acc: 0.4693\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.7889 - acc: 0.4831\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.7396 - acc: 0.4984\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.6957 - acc: 0.5086\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 1.6486 - acc: 0.5241\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.6074 - acc: 0.5334\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.5666 - acc: 0.5469\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.5289 - acc: 0.5555\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 1.4925 - acc: 0.5650\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.4602 - acc: 0.5722\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.4276 - acc: 0.5821\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.3965 - acc: 0.5879\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.3673 - acc: 0.5989\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 1.3403 - acc: 0.6043: 0s - loss: 1.3403 - acc: 0.6\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 1.3149 - acc: 0.6081\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.2891 - acc: 0.6163\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.2642 - acc: 0.6220\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.2424 - acc: 0.6246\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.2217 - acc: 0.6323\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 1.2000 - acc: 0.6360\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.1807 - acc: 0.6420\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.1620 - acc: 0.6458\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.1432 - acc: 0.6489\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.1247 - acc: 0.6554\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.1084 - acc: 0.6580\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0914 - acc: 0.6613\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0768 - acc: 0.6661\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0607 - acc: 0.6712\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0470 - acc: 0.6748\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0344 - acc: 0.6754\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0221 - acc: 0.6798\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0081 - acc: 0.6802\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.9960 - acc: 0.6871\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.9859 - acc: 0.6890\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.9718 - acc: 0.6928\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.9614 - acc: 0.6940\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.9519 - acc: 0.6968\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.9387 - acc: 0.6996\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.9314 - acc: 0.7000\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.9205 - acc: 0.7032\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.9122 - acc: 0.7066\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.9040 - acc: 0.7082\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8922 - acc: 0.7111\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8845 - acc: 0.7145\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8773 - acc: 0.7154\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8706 - acc: 0.7175\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8630 - acc: 0.7171\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.8538 - acc: 0.7185\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8462 - acc: 0.7197\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8373 - acc: 0.7273\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8333 - acc: 0.7238\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8250 - acc: 0.7275\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.8180 - acc: 0.7297\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.8139 - acc: 0.7301\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8076 - acc: 0.7295\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.8024 - acc: 0.7327\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7947 - acc: 0.7346\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.7889 - acc: 0.7351\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7861 - acc: 0.7342\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.7788 - acc: 0.7362\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7750 - acc: 0.7374\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7682 - acc: 0.7395\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7625 - acc: 0.7418\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7607 - acc: 0.7407\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.7553 - acc: 0.743 - 2s 36us/step - loss: 0.7552 - acc: 0.7436\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7505 - acc: 0.7431\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7471 - acc: 0.7420\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7409 - acc: 0.7450\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7390 - acc: 0.7467\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7334 - acc: 0.7481\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7322 - acc: 0.7470\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7273 - acc: 0.7493\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7238 - acc: 0.7502\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7208 - acc: 0.7499\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7180 - acc: 0.7523\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7127 - acc: 0.7512\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7091 - acc: 0.7533\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7058 - acc: 0.7545\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.7035 - acc: 0.7550\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6997 - acc: 0.7545\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6963 - acc: 0.7580\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6935 - acc: 0.7581\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6918 - acc: 0.7560\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6889 - acc: 0.7579\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6854 - acc: 0.7606\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.6830 - acc: 0.7603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da1767fac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "model.fit( x_train, y_train, batch_size = batch_size, epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 45us/step\n",
      "\\ n loss: 0.6512007102012635\n",
      "\\ n accuracy: 0.7736\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate( x_test, y_test) \n",
    "print('\\ n loss:', scores[ 0]) \n",
    "print('\\ n accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Image:  [9 2 1 1 6]\n",
      "Predicted Image : [9, 2, 1, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual Image: \", testY[0:5])\n",
    "predicted = model.predict(x_test[0:5])\n",
    "print(\"Predicted Image :\", [max_index(p) for p in predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9CSqKvpOIVk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGAad54JOIVm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a sequential model \n",
    "lr = 0.03\n",
    "model = Sequential() # the first layer has to specify the dimensions of the input vector \n",
    "model.add( Dense( units = 100, activation ='sigmoid', input_shape =( n_inputs,))) # add dropout layer for preventing overfitting \n",
    "model.add( Dropout( 0.1)) \n",
    "model.add( Dense( units = 100, activation ='sigmoid')) \n",
    "model.add( Dropout( 0.1)) # output layer can only have the neurons equal to the number of outputs \n",
    "model.add( Dense( units = n_classes, activation ='softmax')) # print the summary of our model \n",
    "model.summary() \n",
    "\n",
    "# compile the model \n",
    "model.compile( loss ='categorical_crossentropy', optimizer = SGD(), metrics =['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQ7oIymROIVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 1.8611 - acc: 0.4113\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 1.3339 - acc: 0.6130\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 1.0611 - acc: 0.6691\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.9136 - acc: 0.6965\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.8183 - acc: 0.7222\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.7530 - acc: 0.7399\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.7059 - acc: 0.7529\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6683 - acc: 0.7657\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6401 - acc: 0.7755\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.6185 - acc: 0.7838\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5994 - acc: 0.7898\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5822 - acc: 0.7956\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5757 - acc: 0.7993\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5651 - acc: 0.8026\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5584 - acc: 0.8043\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5521 - acc: 0.8085\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5504 - acc: 0.8106\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5396 - acc: 0.8119\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5356 - acc: 0.8133\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5373 - acc: 0.8125\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5251 - acc: 0.8165\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5257 - acc: 0.8133\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5237 - acc: 0.8164\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5215 - acc: 0.8176\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5172 - acc: 0.8184\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5172 - acc: 0.8163\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5142 - acc: 0.8175\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5149 - acc: 0.8197\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5127 - acc: 0.8187\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5092 - acc: 0.8209\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5132 - acc: 0.8193\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5131 - acc: 0.8212\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5108 - acc: 0.8186\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5061 - acc: 0.8221\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5101 - acc: 0.8189\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5124 - acc: 0.8191\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5170 - acc: 0.8185\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5183 - acc: 0.8184\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5112 - acc: 0.8181\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5094 - acc: 0.8185\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4990 - acc: 0.8224\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5065 - acc: 0.8206\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5069 - acc: 0.8210\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5138 - acc: 0.8191\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5015 - acc: 0.8208\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.5000 - acc: 0.8226\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4975 - acc: 0.8217\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.4905 - acc: 0.8258\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4861 - acc: 0.8269\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4914 - acc: 0.8234\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4836 - acc: 0.8266\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4853 - acc: 0.8284\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4905 - acc: 0.8257\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4793 - acc: 0.8299\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4872 - acc: 0.8269\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4846 - acc: 0.8287\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4808 - acc: 0.8292\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4928 - acc: 0.8250\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4887 - acc: 0.8256\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4779 - acc: 0.8287\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4844 - acc: 0.8276\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4858 - acc: 0.8277\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4873 - acc: 0.8264\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4898 - acc: 0.8261\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4844 - acc: 0.8296\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4846 - acc: 0.8276\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4892 - acc: 0.8276\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4829 - acc: 0.8300\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4783 - acc: 0.8310\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4766 - acc: 0.8302\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4743 - acc: 0.8318\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4751 - acc: 0.8291\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4735 - acc: 0.8305\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4775 - acc: 0.8295\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4813 - acc: 0.8292\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4829 - acc: 0.8287\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4816 - acc: 0.8280\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4810 - acc: 0.8285\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4749 - acc: 0.8302\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4774 - acc: 0.8303\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4792 - acc: 0.8288\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4765 - acc: 0.8310\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4825 - acc: 0.8279\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4835 - acc: 0.8286\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4804 - acc: 0.8289\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4737 - acc: 0.8322\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4822 - acc: 0.8284\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4754 - acc: 0.8304\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4728 - acc: 0.8316\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4660 - acc: 0.8347\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4606 - acc: 0.8364\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4696 - acc: 0.8318\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4662 - acc: 0.8320\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4617 - acc: 0.8365\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4691 - acc: 0.8316\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4686 - acc: 0.8298\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4617 - acc: 0.8342\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4670 - acc: 0.8323\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4609 - acc: 0.8343\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.4617 - acc: 0.8342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da1767f630>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \n",
    "model.fit( x_train, y_train, batch_size = batch_size, epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-O-fFxnOIVt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/step\n",
      "\\ n loss: 0.4615376832723618\n",
      "\\ n accuracy: 0.8341\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate( x_test, y_test) \n",
    "print('\\ n loss:', scores[ 0]) \n",
    "print('\\ n accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiP7IL52OIVw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Image:  [9 2 1 1 6]\n",
      "Predicted Image : [9, 2, 1, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual Image: \", testY[0:5])\n",
    "predicted = model.predict(x_test[0:5])\n",
    "print(\"Predicted Image :\", [max_index(p) for p in predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr2YsZV0OIV0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4ojW6-oOIV2"
   },
   "outputs": [],
   "source": [
    "# Beejal - Done above`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfFGmbZLOIV5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIkbMEN5OIV7"
   },
   "outputs": [],
   "source": [
    "# Beejal - Done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1566711540.3519387"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Execution Time (Seconds):  818.6493380069733\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Execution Time (Seconds): \", (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "R6_ExternalLab_AIML.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
